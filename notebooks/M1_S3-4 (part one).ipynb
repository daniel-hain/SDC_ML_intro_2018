{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorative analysis and data visualization\n",
    "#### Roman Jurowetzki - SDS 2018, S3-4 - 7 September 2018\n",
    "\n",
    "This is a Jupyter notebook which you are going to see and use a lot as an aspiring data scientist. Notebooks can be run with different kernels (Python, R, F# etc.). This one runs Python 3. \n",
    "\n",
    "Notebooks combine Markdown cells for content with computational cells. This allows to \n",
    "\n",
    "In this session we will explore a **real** dataset from the Stanford Open Policing Project. \n",
    "The project is collecting and standardizing data on vehicle and pedestrian stops from law enforcement departments across the US â€” and making that information freely available. They have already gathered 130 million records from 31 state police agencies and have begun collecting data on stops from law enforcement agencies in major cities, as well.\n",
    "\n",
    "You can read more about the project [here](https://openpolicing.stanford.edu)\n",
    "\n",
    "![open police](https://comm.stanford.edu/mm/2017/08/open-policing-project.jpg)\n",
    "\n",
    "\n",
    "#### Exploratory data analysis\n",
    "\n",
    "You can read more about EDA with pandas [here](https://www.datacamp.com/community/tutorials/exploratory-data-analysis-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML('<iframe width=\"800\" height=\"500\" src=\"https://www.youtube-nocookie.com/embed/PelSGxTPlXM?rel=0&amp;controls=0&amp;showinfo=0&amp;start=435\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This session will also be an introduction to the Python data science stack, specifically the __pandas__ and __seaborn__ packages. \n",
    "\n",
    "![python_stack](http://chris35wills.github.io/courses/pydata_stack.png)\n",
    "\n",
    "- Pandas is Python's main library for managing and analysing dataframes. \n",
    "- Seaborn is a high-level library for statistical visualisation. \n",
    "\n",
    "Everything that we do today can be easily transleted in R (using dplys, ggplot etc.) Thus, it is more about the concepts than the particular language.\n",
    "\n",
    "---\n",
    "\n",
    "## Agenda\n",
    "\n",
    "- Import and examine the data for one US state\n",
    "- Preprocess the data (cleaning, adjusting datatypes)\n",
    "- Calculate some simple statistics on different levels of aggregation\n",
    "- Make informative plots\n",
    "- Come up and test *hypotheses*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll download the latest dataset directly from the Stanford server. To do that we use Jupyter's **!** command line magic. Passing a `!` in a code cell, will send the command to shell rather than Python or R. `!wget` will open the GNU Wget Unix/Linux program that downloads content from web servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the semi-raw data\n",
    "#!wget https://stacks.stanford.edu/file/druid:py883nd2578/RI-clean.csv.gz\n",
    "#!wget https://storage.googleapis.com/sds-file-transfer/CT-clean.csv.gz\n",
    "!wget https://storage.googleapis.com/sds-file-transfer/RI-clean.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the file to get a csv\n",
    "!gunzip RI-clean.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import pandas as pd. The appbriviation pd is a convention. We also load the data using pandas's `read_csv` command. Pandas will try to infer the appropriate datatypes dor all columns. We set the `low_memory` argument to `False`, which is often done in cases with possible mixed datatypes. By the way: You if you place the cursor after the '(' after a command and use string+tab, you can read the docummentation for the particular command or function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('RI-clean.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the dataset in several ways, for instance, by checking the first couple of rows or by printing the \"info\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the first 5 (or more/less rows) of each column\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each row is one traffic stop\n",
    "- NaN are missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display overview information for the dataframe\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the DF as an easy alternative to looking up this in info.\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show missing values (mask), cut to the first 10 rows. Note, that python index alsways starts with 0!\n",
    "data.isnull()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The isnull command generates a dataframe with bool (True/False) outputs that you can apply commands on.\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the county_name column is all missing. And thus we can drop it. Also, we probably should drop all cases, where we don't know the time and date of the stop, the gender of the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the fine_grained_location, county_fips, county_name columns.\n",
    "data.drop(['county_name', 'county_fips', 'fine_grained_location'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns where stop_data, time, and driver_gender are missing\n",
    "data.dropna(subset=['stop_date', 'stop_time', 'driver_gender'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check this potential candidate column\n",
    "data.is_arrested.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column should be boolian but for some reason ended up being a string (or object). This is not efficient and limits our options in terms of what we can do with it. Therefore, we will change the datatype.\n",
    "\n",
    "#### On bracket vs. dot notation:\n",
    "\n",
    "In Python you will find 2 notation types. [ ] and .\n",
    "\n",
    "``` ri['is_arrested'] is the same as ri.is_arrested````\n",
    "\n",
    "However, if you assign something and have it on the left side of = you should always use [ ] notation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the is_arrested column a new datatype\n",
    "data['is_arrested'] = data.is_arrested.astype('bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there another column that may suffer from this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting dates, times and index\n",
    "\n",
    "As you can see, stop_date and time are objects. That's not very useful. Let's transform them into a handy date-time-index.\n",
    "\n",
    "First, we will concatenate the two columns into one. Second, we will ask pandas to parse it and set the DF's index as the date and time of the stop. This makes lots of sense, given that each row is an *event*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by concatenating the two string columns into one that we call combined, using str.cat\n",
    "\n",
    "combined = data.stop_date.str.cat(data.stop_time, sep=' ')\n",
    "print(combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides many really nice string options that you definetely should explore. Just set your cursor after str and press tab for a list of options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can for example get dummies from a categorical string variable (here just for the first 10)\n",
    "data.violation[:10].str.get_dummies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a date_and_time column form our combined (Series - basically a DF with only one column). Finally we will set the index of the dateframe to be the column (instead of a normal index). This will open up for many options, e.g. resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the combined series to datetime-format and assign it to a new column\n",
    "data['date_and_time'] = pd.to_datetime(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the standard index by the new column (which will in turn disappear)\n",
    "data.set_index('date_and_time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['district'] = data.county_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data step by step\n",
    "\n",
    "We will start with simple countrs, proportions, averages etc. and move from there to more advanced concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can explore unique values for a column (even if it's a string)\n",
    "data.stop_outcome.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the distinct values\n",
    "data.stop_outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many ways to do the same thing\n",
    "data.groupby('stop_outcome').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts provides a nice proportions option\n",
    "data.stop_outcome.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check distribution by race\n",
    "data.driver_race.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try out some hypotheses\n",
    "\n",
    "One hypothesis could be that the stop_outcome is different for different races. Discrimination?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3 series for different races\n",
    "black = data[data.driver_race == 'Black']\n",
    "white = data[data.driver_race == 'White']\n",
    "hispanic = data[data.driver_race == 'Hispanic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black.stop_outcome.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white.stop_outcome.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hispanic.stop_outcome.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try out to come up with some interesting hypotheses and find answers using the methods that we learned so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are probably asking: Can't we speed this up somehow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You probably will have to \n",
    "#!pip install pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "pandas_profiling.ProfileReport(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering my multiple conditions\n",
    "\n",
    "We can of cause chain filter conditions (you probably learned to do that in tha last session using R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the ( ) around the conditions\n",
    "hispanic_and_arrested = data[(data.driver_race == 'Hispanic')\n",
    "                             & (data.is_arrested == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you are bored by the shape command\n",
    "len(hispanic_and_arrested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also ask for hispanic OR arrested (not sure how much that tells us)\n",
    "hispanic_or_arrested = data[(data.driver_race == 'Hispanic')\n",
    "                             | (data.is_arrested == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hispanic_or_arrested.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rules for filtering\n",
    "\n",
    "- & AND\n",
    "- | OR\n",
    "- Each condition must be surrounded by () and many are possible\n",
    "- == Equality\n",
    "- != Inequality\n",
    "\n",
    "##### Remember, that we are not making any statement about causation. This is purely a correlation exercise (so far!)\n",
    "\n",
    "#### A bit on boolean series\n",
    "\n",
    "True = 1 and False = 0\n",
    "Which means that you can perform calculations on them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of male and female drivers stopped for speeding\n",
    "female_and_speeding = data[(data.driver_gender == 'F') & (data.violation == 'Speeding')]\n",
    "male_and_speeding = data[(data.driver_gender == 'M') & (data.violation == 'Speeding')]\n",
    "\n",
    "# Compute the stop outcomes for drivers (as proportions)\n",
    "print(female_and_speeding.stop_outcome.value_counts(normalize=True))\n",
    "print(male_and_speeding.stop_outcome.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have multiple things that should be shown from one cell's output, it's better to print it out\n",
    "print(data.is_arrested.dtype)\n",
    "print(data.is_arrested.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using groupby to compare groups\n",
    "\n",
    "Rememebre when we compared stop outcome rates by race? Well: That was not very elegant. We can certainly do better using the groupby function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first group by the race and then calculate the mean for the arrested column\n",
    "data.groupby('driver_race').is_arrested.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that black and Hispanic drivers end up more than twice as often arrested than white drivers. But perhaps geography plays a role and perhaps there are some outlier \"bad neighborhoods\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by district and race and calculate the mean of a third factor\n",
    "data.groupby(['district','driver_race']).is_arrested.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which violations lead for the different genders to what rates of arrest?\n",
    "data.groupby(['violation','driver_gender']).is_arrested.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"protective frisk\"\n",
    "Sometimes during stops if a search is conducted, the officer also checks the driver if they have a weapon. This is called a \"protective frisk\".\n",
    "Let's try to figure out if men are frisked more than women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the different search types performed\n",
    "data.search_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting a string\n",
    "As you can see, search type is a multiple choice object/string column. *Incident to Arrest* and *Pribable Cause* are the most commont but combinations are possible. We can use the `str.contains` method to filter to filter out cases of interest. This will return a boolean series, which we can assign to a new varioable 'frisk' in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ask pandas to find out if the string in the search_type column contatins\n",
    "# the sequence \"Protective Frisk\"\n",
    "# We assign the result to a new column that we call \"frisk\"\n",
    "\n",
    "data['frisk'] = data.search_type.str.contains('Protective Frisk', na = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# backup code in case you are not convinced that it worked\n",
    "# this snippet shows the source and the boolean result side by side\n",
    "\n",
    "# generate a data frame by concatenating source and target of the str.contains method. Note, that this concat\n",
    "# of columns or rows is different from the str.cat that we learned before\n",
    "frisks = pd.concat([data.search_type, data.search_type.str.contains('Protective Frisk', na = False)], axis=1)\n",
    "\n",
    "# display the second column using the iloc selector (here useful since we have two columns with the same name)\n",
    "frisks[frisks.iloc[:,1] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['frisk'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do men get frisked more often?\n",
    "data.groupby('driver_gender').frisk.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the datetime index to select data\n",
    "\n",
    "What if you assume that things got better or worse over the years? Or perhaps the time of the day plays a role?  ðŸ“ˆ\n",
    "Remember we assigned a datetime column to our index? \n",
    "In case you need a recap:\n",
    "\n",
    "```python\n",
    "combined = data.stop_date.str.cat(data.stop_time, sep=' ')\n",
    "data['date_and_time'] = pd.to_datetime(combined)\n",
    "data.set_index('date_and_time', inplace=True)\n",
    "```\n",
    "\n",
    "That allows us now to access the time dimension at various levles in our index.\n",
    "\n",
    "```python\n",
    "data.index.day\n",
    "data.index.month\n",
    "data.index.day_name()\n",
    "data.index.month_name()\n",
    "```\n",
    "\n",
    "We can now use that for groupby etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are things getting better or worse over the years?\n",
    "data.groupby(data.index.year).frisk.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(data.index.year).is_arrested.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(data.index.month_name()).is_arrested.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start plotting some of these stats\n",
    "\n",
    "Pandas provides a very easy plotting interface for standard visualisations. For more complex plots, we will be using Seaborn later on.\n",
    "\n",
    "But for now, you can access plotting simply by adding ```.plot()```` after applying some calculation to a dataframe or series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to tell Jupyter to activate inline plots\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] =(12,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. Show the different months that can be accessed\n",
    "figure = data.groupby(data.index.month).is_arrested.mean().plot(kind='bar')\n",
    "\n",
    "plt.xlabel('month')\n",
    "plt.ylabel('arrest rate')\n",
    "plt.title('Monthly mean arrest rate in traffic stops')\n",
    "\n",
    "# saving the plot is as easy as\n",
    "\n",
    "plt.savefig('is_arrested_bymonth.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using resampling to aggregate data rather than group it.\n",
    "\n",
    "But isn't that the same what we just did before? Well, actually it's not the same. Instead of grouping by month (12 months), we will re-aggregate the data for the individual months: Jan 05 - Dec 15.\n",
    "\n",
    "``` python\n",
    "data.is_arrested.resample('M').mean()````\n",
    "\n",
    "More on that here: [Resampling time series data with pandas](http://benalexkeen.com/resampling-time-series-data-with-pandas/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.is_arrested.resample('M').mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.search_conducted.resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['search_conducted'] = data.search_conducted.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the monthly rate of drug-related stops\n",
    "monthly_drug_rate = data.drugs_related_stop.resample('M').mean()\n",
    "\n",
    "# Calculate and save the monthly search rate\n",
    "monthly_search_rate = data.search_conducted.resample('M').mean()\n",
    "\n",
    "# Concatenate the two\n",
    "monthly = pd.concat([monthly_drug_rate,monthly_search_rate], axis='columns')\n",
    "\n",
    "# cut of a few years in the beginning\n",
    "monthly = monthly[monthly.index>=pd.to_datetime('2007-1-1')]\n",
    "\n",
    "# Create subplots from 'annual'\n",
    "monthly.plot(subplots=True)\n",
    "\n",
    "# Display the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crosstabs\n",
    "\n",
    "Crosstabs allow us to explore categorical data considering varous dimensions. Actually, it is the same as grouping by several columns, counting it up with .size() and then bringing the data in wide format (using .unstack())\n",
    "\n",
    "You'll agree that crosstab is easier...however for some reason computationally more expensive ðŸ¤”\n",
    "Sidenote: \n",
    "\n",
    "```%time``` infront of a line of code to measure execution time\n",
    "```%%time``` before executing a code-chunk (for instance a loop or a function which we will cover eventully)\n",
    "\n",
    "```%timeit``` will perform the same 10, 100, 1000 times and give you an average (you never know what your CPU and memory are up to at any moment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time data.groupby([data.driver_race, data.driver_gender]).size().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pd.crosstab(data.driver_race, data.driver_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming categorical in to nummerical data\n",
    "\n",
    "You may have noted the ```stop_duration``` column in our dataset and that it is an ```object``` variable. That means, we can use it as a dimension but not to perform any calculations. What we can do, is map the categories to a reasonable nummerical value using a mapping dictionary the  ```map``` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we should inspect the unique values of the column:\n",
    "# you could use the unique() command but value_counts() is helpful here,\n",
    "# as it can help us identify outlier cases\n",
    "\n",
    "data.stop_duration.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are 6 rows that clearly don't fit and should probably be eliminated\n",
    "We need to filter the dataframe for observations where the ```stop_duration``` hase one of the three values: '0-15 Min', '16-30 Min', '30+ Min'\n",
    "\n",
    "We can use pandas' ```isin``` function here. By the way: If you are doing the opposite \"not in\" query, you can set a ```~``` after the ```[``` and it will inverse your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm lazy: First we can ask pandas to give us a list of possible values that we can copy-paste below into our query\n",
    "data.stop_duration.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step should be pretty familiar\n",
    "\n",
    "data = data[data.stop_duration.isin(['0-15 Min', '16-30 Min', '30+ Min'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's see if the inverse also works:\n",
    "\n",
    "data[~data.stop_duration.isin(['0-15 Min', '16-30 Min', '30+ Min'])].stop_duration.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, challenges pop up all the time and that's why working with data is not a linear or clean process but messy, at times confusing and will require you to look up things on the interenet all the time.\n",
    "\n",
    "Now we can create a mapping dictionary. But wait, we never covered what a dictionary is.\n",
    "\n",
    "A dictionary is one of Python's fundamental data structures. You already met ints, floats, strings. There are also lists, sets, tuples and dictionaries (these are the most common types).\n",
    "\n",
    "For now: A dictionary maps ```keys```to ```values```\n",
    "\n",
    "![dict](https://developers.google.com/edu/python/images/dict.png)\n",
    "\n",
    "You input a key and you get a value, disregarding the order. A dictionary has a slightly weird syntax with curly brackets and : but you'll get used to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our mapping\n",
    "\n",
    "mapping = {'0-15 Min':7.5, '16-30 Min':23, '30+ Min':45}\n",
    "\n",
    "# And use it right away to create a new column\n",
    "data['stop_duration_num'] = data.stop_duration.map(mapping)\n",
    "\n",
    "# a quick check of what we achieved\n",
    "print(data.stop_duration_num[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick intro to loops\n",
    "\n",
    "The great advantage with computers is that they are happy to do boring repetitive stuff for us without getting tired. If we think a bit about the ```map``` function, what it did was to take every observation of ```stop_duration``` in our dataframe and translate it to the corresponding value in our mapping dictionary.\n",
    "\n",
    "Let's try some made up pseudocode:\n",
    "\n",
    "```\n",
    "\n",
    "stop_duration_num = [] #creating an empty list\n",
    "\n",
    "for every observation in data.stop_duration do:\n",
    "    look up corresponding value in the mapping dictionary\n",
    "    append the value to the stop_duration_num list\n",
    "    \n",
    "finally:\n",
    "\n",
    "data['stop_duration_num'] = stop_duration_num list\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "Actually we can write exactly that as a loop\n",
    "\n",
    "```python\n",
    "stop_duration_num_list = []\n",
    "\n",
    "for x in data.stop_duration:\n",
    "    value = mapping[x]\n",
    "    stop_duration_num_list.append(value)\n",
    "\n",
    "# we skip the creation of the new column since we already have it\n",
    "\n",
    "```\n",
    "\n",
    "Aside form for-loop as that one there are also while loops that will do something while some condition is met\n",
    "\n",
    "You can find more on datatypes and loops (iteration) in [this cheat sheet](https://www.theredhillacademy.org.uk/pluginfile.php?file=/15673/block_html/content/Python%20Cheat%20Sheet.pdf)\n",
    "and many other places. Iteration is a core concept in computer science  and will be important in later modules. If you code for the first time in your life, it's a slightly strange concept to get your head around but after some time it becomes second nature. For now, it's good to get some initial feeling for the concept.\n",
    "\n",
    "#### Let's return to our ```stop_duration``` problem\n",
    "\n",
    "Since, we created a nummerical value, we can for instance check if average stop duration is different for different violations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's do everything in one line\n",
    "\n",
    "data.groupby([data.violation_raw]).stop_duration_num.mean().sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cutting intervals\n",
    "\n",
    "```data.driver_age``` contains the (duh) the age of the driver. ```data.driver_age.dtype``` will tell us that it's a continuous nummerical value and thus good for more advanced analysis but perhaps a bit to detailed for exploration (?)\n",
    "\n",
    "More instrumental in that context would be to slice that variable up into ordered categries corresponding to age-populations of interest, say \"teen\", \"20s\", \"30s\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the variable\n",
    "data.driver_age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can slice tha data into some numebr of bins (sometimes useful)\n",
    "data['age_cat'] = pd.cut(data.driver_age,bins=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also provide labels\n",
    "labels=[\"teen\", \"20s\", \"30s\", \"40s\", \"50+\"]\n",
    "data['age_cat'] = pd.cut(data.driver_age, bins=5, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also provide custom intervals\n",
    "# Unfortunately (not sure why) there is no way to do both together\n",
    "bins = pd.IntervalIndex.from_tuples([(10, 20), (20, 30), (30, 40), (40,50), (50,100)])\n",
    "data['age_cat'] = pd.cut(data.driver_age, bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['age_cat'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can plot that but perhaps let's modify a bit first\n",
    "table = pd.crosstab(data.driver_race, data.age_cat)\n",
    "\n",
    "# keep only minority races\n",
    "table = table.iloc[:3,]\n",
    "\n",
    "# let's plot some stacked bars\n",
    "table.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enters weather data\n",
    "\n",
    "To be precise: Local climatological data from https://www.ncdc.noaa.gov/\n",
    "\n",
    "I put the data in a Google Cloud bucket: https://storage.googleapis.com/sds-file-transfer/RI-weather.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather[['AWND', 'WSF2']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weather[['AWND', 'WSF2']].plot(kind='box') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['WDIFF'] = weather.WSF2 - weather.AWND\n",
    "weather.WDIFF.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weather.WDIFF.plot(kind='hist', bins=50)\n",
    "plt.savefig('fig1.pdf') # would like to save it for later? Pass this line in the same cell and you'll get the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing Seaborn\n",
    "\n",
    "Seaborn is a great project with the mission to make statistical plots easier in Python. You can find more on their [Homepage](https://seaborn.pydata.org/index.html).\n",
    "\n",
    "Datacamp created [this cheat sheet](https://www.datacamp.com/community/blog/seaborn-cheat-sheet-python) that summarizes most important functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already imported seaborn (without talking a lot about it). Let's do it again, which is not a problem\n",
    "\n",
    "import seaborn as sns # sns is the conventional abbriviation\n",
    "sns.set(style=\"darkgrid\") # Darkgrid is a nice ggplot (R) - like style that looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's do the same what we just did but now using Searborn\n",
    "# We replace the standard histogram by a Distribution plot (same same)\n",
    "sns.distplot(weather.WDIFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read much more about all the different things that you can do with distplots [here](https://seaborn.pydata.org/generated/seaborn.distplot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = sns.distplot(weather.WDIFF, hist=False,\n",
    "             rug=True,\n",
    "             kde_kws={'shade':True})\n",
    "\n",
    "fig.figure.savefig('fig.pdf') # In case you would like to keep it :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further examining the weather data\n",
    "weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's select just a sequence of columns 'from-to'\n",
    "temp = weather.loc[:, 'TAVG':'TMAX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we say something about average temperature and average wind?\n",
    "\n",
    "sns.jointplot(weather.TAVG, weather.AWND, kind=\"hex\", color=\"#4CB391\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data types advanced\n",
    "\n",
    "We already used the map function to transform strings to nummerical values.\n",
    "Now, let's try to create categories. This is not always necessary but nice to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'0-15 Min':'short', '16-30 Min':'medium', '30+ Min':'long'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stop_length'] = data.stop_duration.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.stop_length.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to initiate this new data format first importing it\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# Define our categories\n",
    "cats = ['short', 'medium', 'long']\n",
    "\n",
    "# And define the specific cateogory type (this is useful for survey data that is ordered)\n",
    "cat_type = CategoricalDtype(categories=cats, ordered=True)\n",
    "\n",
    "# Finally let's asign it\n",
    "data['stop_length'] = data.stop_length.astype(cat_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.stop_length.memory_usage(deep=True)\n",
    "\n",
    "# The first thing we notice --> this type is much more memory friendly. This is good when moving towards big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also now we see that pandas knows that short is short < medium < long\n",
    "data.stop_length.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Any relation between stop duration and likelihood to get arrested?\n",
    "data.groupby('stop_length').is_arrested.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to our weather data\n",
    "\n",
    "Do you think that arrest rates or certain violations are related to weather?\n",
    "For that we need to connect our weather data with the stops data\n",
    "\n",
    "We will merge the two dataframes on the date index, as weather data is available daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's prepare the weather data first, by parsing the date-column\n",
    "weather.DATE = pd.to_datetime(weather.DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to do the same with our stops data and its stop_date column\n",
    "data.stop_date = pd.to_datetime(data.stop_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's merge\n",
    "\n",
    "data_weather = pd.merge(data, weather, left_on='stop_date', right_on='DATE', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas merge command is fairly simple:\n",
    "\n",
    "We start with the left dataframe \"data\" in our case, then the right \"weather\", then pass the left and the right key and finally the merger type: Here \"left\", meaning that we would like to keep the left as it is and multiply the right on top of it (if that makes sense)\n",
    "\n",
    "![merge](http://www.datasciencemadesimple.com/wp-content/uploads/2017/09/join-or-merge-in-python-pandas-1.png)\n",
    "\n",
    "And if you still think merging is mysterious then you should check out this [youtube tutorial](https://youtu.be/h4hOPGo4UVU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_weather.set_index('date_and_time', inplace=True)\n",
    "data_weather.index = data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a not totally crazy assumption that police stops are shorter if it's cold...maybe. Let's check that.\n",
    "\n",
    "The syntax is a bit more advanced but I'll do my best to explain. We will control for the violation type, assuming that different violations lead to different durations by their nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to aggregate the data first on 2 levels: Time and Violation.\n",
    "# For this we need to use pandas Grouper module (I also had to look it up)\n",
    "violation_duration = data_weather.groupby([pd.Grouper(key='stop_date', freq='D'), 'violation']).stop_duration_num.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Such groupby commands deliver a multi-indexed series. That's not useful for further work.\n",
    "# But we can transform them into Dataframes with various index levels turning into columns.\n",
    "violation_duration = pd.DataFrame(violation_duration).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the temperature data\n",
    "avg_temp = data_weather.resample('D').TAVG.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_temp = pd.DataFrame(avg_temp).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's merge both DFs\n",
    "search_temp = pd.merge(violation_duration, avg_temp, left_on='stop_date', right_on='date_and_time', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We plot our resulting DF using Seaborn using the col argument to create subplots for the different violation types\n",
    "sns.lmplot('stop_duration_num', 'TAVG', data=search_temp[search_temp.stop_duration_num < 20], col = 'violation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmaps are another useful visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should by now be able to read this without problems\n",
    "sns.heatmap(pd.crosstab(data.driver_race, data.driver_gender, values=data.is_arrested, aggfunc='mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can make this heatmap more informative using some of Seaborn's functionality\n",
    "\n",
    "sns.heatmap(pd.crosstab(data.driver_race, data.driver_gender, values=data.is_arrested, aggfunc='mean'),\n",
    "            annot=True, cmap=\"YlGnBu\", cbar=False, linewidths=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search discrimination?\n",
    "\n",
    "Finally we would like to find out if searches are conducted more often for some races."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to create a boolean variable for searches\n",
    "data['search_conducted'] = data.search_conducted.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the search rate by race\n",
    "search_rates = pd.crosstab(data.district, data.driver_race, data.search_conducted, aggfunc='mean')\n",
    "\n",
    "### Exactly the same\n",
    "# search_rates = data.groupby(['district','driver_race']).search_conducted.mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches_per_district = data.groupby('district').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_rates_per_district = pd.concat([search_rates, searches_per_district], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_rates_per_district['stops'] = search_rates_per_district[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_rates_per_district"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some more advanced plotting:\n",
    "\n",
    "Can you understand what's going on here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "ax = plt.axes()\n",
    "ax.set_title('White vs. Hispanic Search Rate')\n",
    "plot = sns.scatterplot(x=\"White\", y=\"Hispanic\", size='stops', hue=\"stops\", data=search_rates_per_district)\n",
    "plt.axis([0,0.085,0,0.085])\n",
    "plot.plot(plot.get_xlim(), plot.get_ylim(), ls=\"--\", c=\".3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "ax = plt.axes()\n",
    "ax.set_title('White vs. Black Search Rate')\n",
    "plot = sns.scatterplot(x=\"White\", y=\"Black\", size='stops', hue=\"stops\", data=search_rates_per_district)\n",
    "plt.axis([0,0.085,0,0.085])\n",
    "plot.plot(plot.get_xlim(), plot.get_ylim(), ls=\"--\", c=\".3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_rates = data.groupby(['district','driver_race']).contraband_found.sum() / data.groupby(['district','driver_race']).search_conducted.sum() \n",
    "found_rates_per_district = pd.concat([found_rates.unstack(),data.groupby('district').search_conducted.sum()], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_rates_per_district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "ax = plt.axes()\n",
    "ax.set_title('White vs. Black Find Rate')\n",
    "plot = sns.scatterplot(x=\"White\", y=\"Black\", size='search_conducted', hue=\"search_conducted\", data=found_rates_per_district)\n",
    "plt.axis([0,0.5,0,0.5])\n",
    "plot.plot(plot.get_xlim(), plot.get_ylim(), ls=\"--\", c=\".3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "ax = plt.axes()\n",
    "ax.set_title('White vs. Hispanic Find Rate')\n",
    "plot = sns.scatterplot(x=\"White\", y=\"Hispanic\", size='search_conducted', hue=\"search_conducted\", data=found_rates_per_district)\n",
    "plt.axis([0,0.5,0,0.5])\n",
    "plot.plot(plot.get_xlim(), plot.get_ylim(), ls=\"--\", c=\".3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
